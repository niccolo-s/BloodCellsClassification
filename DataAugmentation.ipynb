{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dw_-hFm6bjY6"
   },
   "source": [
    "## Connect Colab to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24688,
     "status": "ok",
     "timestamp": 1732183438056,
     "user": {
      "displayName": "Fabio Romagnoli",
      "userId": "11084294309182946672"
     },
     "user_tz": -60
    },
    "id": "y2S4GWr3Uoa8",
    "outputId": "7d92c371-d1da-41b9-e365-0a0dccd3465d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /gdrive\n",
      "/gdrive/My Drive/Homework 1\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/gdrive')\n",
    "%cd /gdrive/My Drive/Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1441,
     "status": "ok",
     "timestamp": 1732181035260,
     "user": {
      "displayName": "Fabio Romagnoli",
      "userId": "11084294309182946672"
     },
     "user_tz": -60
    },
    "id": "Ras_sZqnOfvS",
    "outputId": "30ae2b4c-008a-4790-84cf-c97cb657ac92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "tensorflow==2.17.0\n",
    "keras==3.4.1\n",
    "keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 53935,
     "status": "ok",
     "timestamp": 1732181089174,
     "user": {
      "displayName": "Fabio Romagnoli",
      "userId": "11084294309182946672"
     },
     "user_tz": -60
    },
    "id": "FDxQTEkpOeTD",
    "outputId": "14b3ed1b-396f-482d-e6af-16380f4e8f1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.17.0 (from -r requirements.txt (line 1))\n",
      "  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting keras==3.4.1 (from -r requirements.txt (line 2))\n",
      "  Downloading keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting keras_cv (from -r requirements.txt (line 3))\n",
      "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.68.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (2.17.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17.0->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1->-r requirements.txt (line 2)) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1->-r requirements.txt (line 2)) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras==3.4.1->-r requirements.txt (line 2)) (0.13.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras_cv->-r requirements.txt (line 3)) (2024.9.11)\n",
      "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras_cv->-r requirements.txt (line 3)) (4.9.7)\n",
      "Collecting keras-core (from keras_cv->-r requirements.txt (line 3))\n",
      "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras_cv->-r requirements.txt (line 3)) (0.3.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (0.45.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17.0->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_cv->-r requirements.txt (line 3)) (4.66.6)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_cv->-r requirements.txt (line 3)) (0.1.8)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.4.1->-r requirements.txt (line 2)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras==3.4.1->-r requirements.txt (line 2)) (2.18.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (8.1.7)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (4.2.1)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (2.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (5.9.5)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (17.0.0)\n",
      "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (0.10.2)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (0.5.1)\n",
      "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (1.10.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (2024.10.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (6.4.5)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.4.1->-r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17.0->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv->-r requirements.txt (line 3)) (1.66.0)\n",
      "Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: keras-core, keras, tensorflow, keras_cv\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.5.0\n",
      "    Uninstalling keras-3.5.0:\n",
      "      Successfully uninstalled keras-3.5.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.17.1\n",
      "    Uninstalling tensorflow-2.17.1:\n",
      "      Successfully uninstalled tensorflow-2.17.1\n",
      "Successfully installed keras-3.4.1 keras-core-0.1.7 keras_cv-0.9.0 tensorflow-2.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0M591Qw9X6TS"
   },
   "source": [
    "## Model Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1732181089175,
     "user": {
      "displayName": "Fabio Romagnoli",
      "userId": "11084294309182946672"
     },
     "user_tz": -60
    },
    "id": "r4-AjFJzUB0X",
    "outputId": "3bb30684-d352-4326-ae46-dff8fd4bb87a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_M7_dataAug_v2\n"
     ]
    }
   ],
   "source": [
    "mark = '7'\n",
    "description = 'dataAug'\n",
    "version = '2'\n",
    "\n",
    "model_name = \"F_\" + 'M' + mark + '_' + description + '_' + 'v' + version\n",
    "print(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7IqZP5Iblna"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11190,
     "status": "ok",
     "timestamp": 1732181100353,
     "user": {
      "displayName": "Fabio Romagnoli",
      "userId": "11084294309182946672"
     },
     "user_tz": -60
    },
    "id": "CO6_Ft_8T56A",
    "outputId": "563996e6-7429-415b-ceb4-1e90a8d3a083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n",
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "seed = 42\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import setuptools.dist\n",
    "\n",
    "# Set environment variables before importing modules\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "\n",
    "# Import necessary modules\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set seeds for random number generators in NumPy and Python\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import regularizers\n",
    "import keras_cv\n",
    "\n",
    "# Set seed for TensorFlow\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# Reduce TensorFlow verbosity\n",
    "tf.autograph.set_verbosity(0)\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(tf.__version__)\n",
    "print(tfk.__version__)\n",
    "\n",
    "# Import other libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "# Configure plot display settings\n",
    "sns.set(font_scale=1.4)\n",
    "sns.set_style('white')\n",
    "plt.rc('font', size=14)\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import albumentations as A\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIllKv7QY3HZ"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeKuzWYWh5sW"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = np.load('./training_set_cleaned.npz')\n",
    "X = data['images']\n",
    "y = data['labels']\n",
    "\n",
    "# Define the mapping for the labels\n",
    "label_names = {\n",
    "    0: \"Basophil\",\n",
    "    1: \"Eosinophil\",\n",
    "    2: \"Erythroblast\",\n",
    "    3: \"Immature granulocytes\",\n",
    "    4: \"Lymphocyte\",\n",
    "    5: \"Monocyte\",\n",
    "    6: \"Neutrophil\",\n",
    "    7: \"Platelet\"\n",
    "}\n",
    "\n",
    "# Convert labels to categorical format using one-hot encoding\n",
    "y = tfk.utils.to_categorical(y)\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X,y, test_size=0.05, random_state=seed, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val= train_test_split(X_train_val, y_train_val, test_size=0.10, random_state=seed, stratify=y_train_val)\n",
    "\n",
    "\n",
    "# Input shape for the model\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Output shape for the model\n",
    "output_shape = y_train.shape[1]\n",
    "\n",
    "del X_train_val, y_train_val, X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXTse7uWwf6q"
   },
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAIyR0eCxs5o"
   },
   "source": [
    "### Support function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNDATmN3-ijG"
   },
   "outputs": [],
   "source": [
    "def sampler(X, y, strategy, seed):\n",
    "    # Flatten the image data\n",
    "    n_samples, height, width, channels = X.shape\n",
    "    X_flat = X.reshape(n_samples, -1)  # Shape: (n_samples, height*width*channels)\n",
    "    # Ensure labels/ are 1D\n",
    "    y_flat = np.argmax(y, axis=1)  # Shape: (n_samples,)\n",
    "    if strategy == 'o':\n",
    "        sampler = RandomOverSampler(random_state=seed)\n",
    "    elif strategy == 'u':\n",
    "        sampler = RandomUnderSampler(sampling_strategy='auto', random_state=seed)\n",
    "    elif strategy == 'n':\n",
    "        return X, y\n",
    "    else:\n",
    "        raise ValueError(\"Invalid type. Use 'o' or 'u'.\")\n",
    "    X_resampled_flat, y_resampled = sampler.fit_resample(X_flat , y_flat)\n",
    "    # Reshape the resampled data back to the original image dimensions\n",
    "    X_resampled = X_resampled_flat.reshape(-1, height, width, channels)\n",
    "    y_resampled = tfk.utils.to_categorical(y_resampled, num_classes=y.shape[1])\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "def augment_data(X,y, transform):\n",
    "    dataset = {\"images\": tf.convert_to_tensor(X,dtype='float32'), \"labels\": tf.convert_to_tensor(y,dtype='float32')}\n",
    "    augmented = transform(dataset)\n",
    "    return augmented[\"images\"],augmented[\"labels\"]\n",
    "\n",
    "def generator(X,y,transform, strategy, seed):\n",
    "    Xs,ys = sampler(X, y,strategy, seed=seed)\n",
    "    return augment_data(Xs, ys, transform)\n",
    "\n",
    "def view_images(dataset):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    for i in range(20):\n",
    "        ax = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(dataset[random.randint(0, len(dataset)-1)].astype('uint8'))\n",
    "    plt.show()\n",
    "\n",
    "def cutMix_mixUp(seed):\n",
    "    # Cut and mix up do be created as extra\n",
    "    cutMix = keras_cv.layers.CutMix(seed=seed)\n",
    "    mixUp = keras_cv.layers.MixUp(seed=seed)\n",
    "    randAugment = keras_cv.layers.RandAugment(value_range=(0, 255),augmentations_per_image= 3,\n",
    "                                              magnitude=0.1,magnitude_stddev=0.03, rate = 0.5,seed=seed, geometric=False)\n",
    "    def apply(samples):\n",
    "      samples = cutMix(samples)\n",
    "      samples = mixUp(samples)\n",
    "      samples = randAugment(samples)\n",
    "      return samples\n",
    "\n",
    "    return apply\n",
    "\n",
    "def augumentation(seed):\n",
    "  augMix = keras_cv.layers.AugMix(value_range=(0,255),num_chains=3,seed=seed)\n",
    "\n",
    "  # Extra pipeline for strange augmentations\n",
    "  channelShuffle = keras_cv.layers.ChannelShuffle(seed=seed)\n",
    "  gridMask = keras_cv.layers.GridMask(seed=seed)\n",
    "  greyScale = keras_cv.layers.Grayscale(output_channels=3)\n",
    "  extraLayers = [channelShuffle,gridMask,greyScale]\n",
    "  extraPipeline = keras_cv.layers.RandomAugmentationPipeline(layers=extraLayers, augmentations_per_image=1,seed=seed)\n",
    "\n",
    "  # Take layers of random augment to reduce deformations overall\n",
    "  randAugmentLayers = keras_cv.layers.RandAugment.get_standard_policy(value_range= (0,255),\n",
    "                                                                      magnitude=0.5, magnitude_stddev=0.15, geometric=True, seed = seed)\n",
    "\n",
    "  layers = randAugmentLayers + [augMix, extraPipeline]\n",
    "  pipeline = keras_cv.layers.RandomAugmentationPipeline(layers=layers, augmentations_per_image=3,seed=seed)\n",
    "\n",
    "  return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3-ur_aoaM8Y"
   },
   "source": [
    "#### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLfoLpXKM8qS"
   },
   "outputs": [],
   "source": [
    "aug1 = augumentation(seed = 2)\n",
    "X_t1, y_t1 = generator(X_train, y_train, aug1, 'n', seed = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xfO6D1OXM-HB"
   },
   "outputs": [],
   "source": [
    "aug2 = augumentation(seed = 31)\n",
    "X_t2, y_t2 = generator(X_train,y_train, aug2, 'n', seed = 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x60XKmyDAf2c"
   },
   "outputs": [],
   "source": [
    "randAugmentStd = keras_cv.layers.RandAugment(value_range=(0, 255) ,seed = 37)\n",
    "X_t3, y_t3 = generator(X_train, y_train, randAugmentStd, 'u', seed = 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-07gvtsL_ck"
   },
   "outputs": [],
   "source": [
    "aug3 = cutMix_mixUp(seed = 73)\n",
    "X_t4, y_t4 = generator(X_train,y_train, aug3, 'n', seed = 73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fSS9_PZNMC1u"
   },
   "outputs": [],
   "source": [
    "# concatenate X_t1 X_t2 etc\n",
    "Xa_train = np.concatenate((X_train, X_t1, X_t2, X_t3, X_t4))\n",
    "ya_train = np.concatenate((y_train, y_t1, y_t2, y_t3,y_t4))\n",
    "np.savez('train.npz', images=Xa_train, labels=ya_train)\n",
    "\n",
    "del X_t1, X_t2, X_t3,X_t4, y_t1, y_t2, y_t3, y_t4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipkMpXeIaSUu"
   },
   "source": [
    "#### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjvgGyniMEIJ"
   },
   "outputs": [],
   "source": [
    "aug_val = augumentation(seed = 127)\n",
    "\n",
    "Xt_val, yt_val = generator(X_val, y_val, aug_val,'n',seed = 127)\n",
    "\n",
    "np.savez('val.npz', images=Xt_val, labels=yt_val)\n",
    "\n",
    "del Xt_val, yt_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLgJR0v9aVol"
   },
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mhKmbF2NL0F"
   },
   "outputs": [],
   "source": [
    "randAugmentStd = keras_cv.layers.RandAugment(value_range=(0, 255),seed=179)\n",
    "Xt_test, yt_test = generator(X_test, y_test, randAugmentStd,'n',seed = 179)\n",
    "\n",
    "np.savez('test.npz', images=Xt_test, labels=yt_test)\n",
    "\n",
    "del Xt_test, yt_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NL-trFs5LDDi",
    "outputId": "606a4a5d-dcea-4340-ad5f-5b68e5fe0f30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created zip archive: datasets.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Zip the .npz files\n",
    "files_to_zip = ['train.npz', 'val.npz', 'test.npz']\n",
    "output_zip_file = 'datasets.zip'\n",
    "\n",
    "with zipfile.ZipFile(output_zip_file, 'w') as zipf:\n",
    "    for file in files_to_zip:\n",
    "        zipf.write(file)\n",
    "print(f\"Created zip archive: {output_zip_file}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "10lScSIn5K22yGeb7ydrnIjJP4OBJlpOZ",
     "timestamp": 1731407330300
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
